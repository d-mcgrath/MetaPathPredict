# create official MetaPredict SQL database --------------------------------

# open up an interactive node on Poseidon

#load reqd libraries
library(tidyverse)
library(furrr)
library(RSQL)
library(RSQLite)

#set parallel processing parameters
plan(multicore, workers = 35) #2700 * 1024 ^2 - 2.7Gb per core, 35 cores total
options(future.globals.maxSize = 2831155200) #set maximum memory per parallal worker

setwd('/vortexfs1/omics/pachiadaki/dgellermcgrath/ensemble/rdata/final_ensemble_models_july_2022')
ens_mods293 = readRDS('/vortexfs1/omics/pachiadaki/dgellermcgrath/ensemble/rdata/auxiliary_r_data/final_293_stacked_ensemble_names.rds')

#
models = list.files()
models = models[str_detect(models, paste0(ens_mods293, collapse = '|'))]

model_names = str_replace(models, '(M\\d{5})_.*', '\\1')


# fn to remove xgboost redundant information
shrink_xgb_recipe = function(.data) {
  #names of features + response var included in final model
  varsToRetain = names(.data$pre$mold$blueprint$recipe$levels)

  #remove bloated xgboost recipe sections of ensemble models
  .data$pre$actions$recipe$recipe$var_info =
    .data$pre$actions$recipe$recipe$var_info %>%
    filter(variable %in% varsToRetain)

  .data$pre$actions$recipe$recipe$term_info =
    .data$pre$actions$recipe$recipe$term_info %>%
    filter(variable %in% varsToRetain)

  .data$pre$actions$recipe$recipe$template =
    .data$pre$actions$recipe$recipe$template %>%
    select(all_of(varsToRetain))

  .data$pre$mold$blueprint$recipe$var_info =
    .data$pre$mold$blueprint$recipe$var_info %>%
    filter(variable %in% varsToRetain)

  .data$pre$mold$blueprint$recipe$last_term_info =
    .data$pre$mold$blueprint$recipe$last_term_info %>%
    filter(variable %in% varsToRetain)

  .data$pre$mold$blueprint$recipe$orig_lvls =
    .data$pre$mold$blueprint$recipe$orig_lvls[varsToRetain]

  .data$pre$mold$blueprint$ptypes$predictors =
    .data$pre$mold$blueprint$ptypes$predictors %>%
    select(varsToRetain[2:length(varsToRetain)])

  return(.data)
}


# fn to remove neural network redundant information
shrink_nnet_recipe = function(.data) {
  #names of features + response var included in final model
  varsToRetain = .data$pre$mold$blueprint$recipe$term_info$variable

  .data$pre$actions$recipe$recipe$var_info =
    .data$pre$actions$recipe$recipe$var_info %>%
    filter(variable %in% varsToRetain)

  .data$pre$actions$recipe$recipe$term_info =
    .data$pre$actions$recipe$recipe$term_info %>%
    filter(variable %in% varsToRetain)

  .data$pre$actions$recipe$recipe$template =
    .data$pre$actions$recipe$recipe$template %>%
    select(all_of(varsToRetain))

  .data$pre$mold$blueprint$recipe$var_info =
    .data$pre$mold$blueprint$recipe$var_info %>%
    filter(variable %in% varsToRetain)

  .data$pre$mold$blueprint$recipe$last_term_info =
    .data$pre$mold$blueprint$recipe$last_term_info %>%
    filter(variable %in% varsToRetain)

  .data$pre$mold$blueprint$recipe$orig_lvls =
    .data$pre$mold$blueprint$recipe$orig_lvls[varsToRetain]

  .data$pre$mold$blueprint$ptypes$predictors =
    .data$pre$mold$blueprint$ptypes$predictors %>%
    select(varsToRetain[2:length(varsToRetain)])

  return(.data)
}


# remove unneeded model definitions from ensemble model objects
shrink_modeldefs = function(.data) {

  ## xgboost
  .data$model_defs$xgboost_tune$pre$actions$recipe$recipe$var_info = NULL
  .data$model_defs$xgboost_tune$pre$actions$recipe$recipe$term_info = NULL
  .data$model_defs$xgboost_tune$pre$actions$recipe$recipe$template = NULL

  ## nnet
  .data$model_defs$nnet_tune$pre$actions$recipe$recipe$var_info = NULL
  .data$model_defs$nnet_tune$pre$actions$recipe$recipe$term_info = NULL
  .data$model_defs$nnet_tune$pre$actions$recipe$recipe$template = NULL

  return(.data)
}


#fn to load a model, and remove info not reqd for predictions on new data
load_and_trim = function(model) {

  mod = readRDS(model)

  mod$member_fits[str_detect(names(mod$member_fits), 'xgboost')] =
    mod$member_fits[str_detect(names(mod$member_fits), 'xgboost')] %>%
    map(~ shrink_xgb_recipe(.x))

  mod$member_fits[str_detect(names(mod$member_fits), 'nnet')] =
    mod$member_fits[str_detect(names(mod$member_fits), 'nnet')] %>%
    map(~ shrink_nnet_recipe(.x))

  mod = shrink_modeldefs(mod)

  # 12/5/2022 addition - removing problematic 'terms' data
  mod$coefs$preproc$terms = NULL

  #assign(model_name, mod, envir = .GlobalEnv)
  #rm(mod)

  #return(get(model_name))
  #return(assign(model_name, mod))

  return(mod)
}

# load and trim down models in parallel, 35 at a time
# use 'walk' instead of 'map' bc we are just loading & trimming model objects
#future_walk2(models, model_names, ~ assign(.y, load_and_trim(.x), envir = .GlobalEnv))
#the above command doesn't load anything; not sure how to assign from fn

# this below command gets killed - seems to run out of memory; have to try something else
# for (x in seq_along(models)) {
#   assign(model_names[x], load_and_trim(models[x]))
# }




## work on additional neural network models

nnet_models_dir = '/vortexfs1/omics/pachiadaki/dgellermcgrath/ensemble/rdata/final_butchered_nnet_models'






# create empty SQLite database called "model_sqlite.db"
model_db <- RSQLite::dbConnect(drv = RSQLite::SQLite(),
                      dbname = 'model_sqlite.db') #"/vortexfs1/omics/pachiadaki/dgellermcgrath/ensemble/sql_db/final_model_database_sqlite_dec_4_2022.db")

# create a table in SQL database: col 1 = model name, col 2 = raw model
# raw model is basically the whole model saved as a binary "blob"
RSQLite::dbSendQuery(conn = model_db,
            "CREATE TABLE models (model_name CHARACTER, raw_model BLOB)")

# iteratively input each model name and serialized model object into the
#SQL table
# for (x in model_names) {
#   RSQLite::dbExecute(model_db,
#                      'INSERT INTO models VALUES (:model_name, :raw_model)',
#                      params = list(
#                        model_name = x,
#                        raw_model = list(serialize(get(x), NULL))
#                      ))
# }


#' for each model:
  #' load the model; trim unneccessary components;
  #' save it as a row in the SQL table; remove model from memory

models = 'gx'
model_names = 'gx'
for (x in seq_along(models)) {
  assign(model_names[x], load_and_trim(models[x]))

  RSQLite::dbExecute(model_db,
                     'INSERT INTO models VALUES (:model_name, :raw_model)',
                     params = list(
                       model_name = model_names[x],
                       raw_model = list(serialize(get(model_names[x]), NULL))
                     ))

  rm(list = model_names[x])
}



RSQLite::dbExecute(model_db,
                   'INSERT INTO models VALUES (:model_name, :raw_model)',
                   params = list(
                     model_name = 'M00001',
                     raw_model = list(serialize(M00001, NULL))
                   ))




g = serialize(M00001, NULL)

RSQLite::dbExecute(model_db,
                   'INSERT INTO models VALUES (:model_name, :raw_model)',
                   params = list(
                     model_name = 'gx',
                     raw_model = list(serialize(gx, NULL))
                   ))






# create a reference obj to the database using dbplyr
model_db_ref = tbl(model_db, 'models')





# restart R session; then connect to SQL db and try loading a model -------

library(tidyverse)
library(tidymodels)
library(stacks)
library(recipeselectors)


#check object size
size = function(.data) {
  .data %>%
    object.size() %>%
    format(units = 'Mb')
}

# function to properly index and then unserialize a raw model "blob"
unserialize_model = function(.data) {
  unserialize(.data[[1]])
}

#connect to the SQL database
model_db <- RSQLite::dbConnect(drv = RSQLite::SQLite(),
                               dbname = "/vortexfs1/omics/pachiadaki/dgellermcgrath/ensemble/sql_db/model_sqlite.db")

#set up a reference to the database
model_db_ref = tbl(model_db, 'models')

# code to extract a specified raw model from the SQL database and
# then unserialize it/load it into RAM
modelx = model_db_ref %>%
  filter(model_name == 'gx') %>%
  collect() %>%
  pull(raw_model) %>%
  unserialize_model()

# works
